{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df725b0e-0159-4c0c-87ad-46f854039029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0\n",
    "# if you haven't installed wget, uncomment the below command and run this cell\n",
    "#!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "531f1985-c045-485e-b92f-beaca215c168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime as dt\n",
    "import time\n",
    "import wget\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "57396ec8-bdb0-4c0c-9136-11eda975646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "####\n",
    "# where are you scraping?\n",
    "# urls = \"https://guides.library.fresnostate.edu/agbs120\" # if using individual website\n",
    "\n",
    "## if list of URLs\n",
    "# Open the file in read mode\n",
    "with open('libguides-url-list.txt', 'r') as file:\n",
    "    urls = file.read()\n",
    "file.close()    \n",
    "urls = urls.split('\\n')\n",
    "\n",
    "\n",
    "####\n",
    "# where do you want to save locally?\n",
    "\n",
    "# save_loc is redundant, but a reminder that you should have locally folders that reflect the years, because that's where below is saving to\n",
    "save_loc = \"libguides\" \n",
    "\n",
    "# what kind of prefix do you want on your files\n",
    "save_prefix = \"libguides_\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3850fca0-90b4-40cc-9b85-a5f422d77484",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_saved = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9c84fa-011b-4204-8682-e7549982cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in urls:\n",
    "    if url in urls_saved:\n",
    "        print(\"in saved, do nothing1\")\n",
    "    elif url==\"\":\n",
    "        print(\"url is empty\")\n",
    "    else:\n",
    "        webpage = requests.get(url)\n",
    "        soup = BeautifulSoup(webpage.content, 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            data_target = link.get('href')\n",
    "        #        print(data_target)\n",
    "            time.sleep(.25) # be kind, don't look like a DDOS attack\n",
    "            if not(data_target):\n",
    "                print(\"url is empty\")\n",
    "            else: \n",
    "                if (\"guides.library.fresnostate.edu\") in data_target:\n",
    "                #        print(\"is libguide\", data_target)\n",
    "                    if data_target[-1]==\"/\":\n",
    "                #            print(\"strip char\")\n",
    "                        data_target = data_target[:-1]\n",
    "                            \n",
    "                    if data_target in urls_saved:\n",
    "                        print(\"in saved, do nothing2\") \n",
    "                    else:\n",
    "                        if ((\".php\" in data_target) and not(\"c.php\" in data_target)) or (\"/database/\" in data_target) or (\"/az/\" in data_target):  \n",
    "                            print(\"some other type of page, profile/subject/database page, do nothing:\", data_target)\n",
    "                        else:\n",
    "                            urls_saved.append(data_target)\n",
    "                            wget_url = data_target\n",
    "                            wget_save = \"./libguides/\"+save_prefix+data_target.replace(\"https://guides.library.fresnostate.edu\",\"\").replace(\"/c.php?g=\",\"\").replace(\"/\",\"\").replace(\"&p=\",\"p_\")\n",
    "                            if not((wget_save[-3:]==\"htm\") or (wget_save[-4:]==\"html\")):\n",
    "                                wget_save = wget_save+\".html\"\n",
    "                            \n",
    "                            if os.path.exists(wget_save):\n",
    "                                print(wget_save+\" already exists. Did NOT save.\")\n",
    "                            else:\n",
    "                                r = requests.head(data_target)\n",
    "                                if (r.status_code == 200):\n",
    "                                    print(\"SAVING \"+ wget_url+ \" at local location: \"+wget_save)\n",
    "                                    wget.download(wget_url,wget_save)\n",
    "                                    time.sleep(.25) # be kind, don't look like a DDOS attack\n",
    "\n",
    "print(\"fin.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea28a9e-34a2-4f0c-87f6-99f6b8e80870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100e233f-045b-4ab9-b514-f1f00c397b52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8736df-a299-439e-b9d1-9f6bc6c98e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
